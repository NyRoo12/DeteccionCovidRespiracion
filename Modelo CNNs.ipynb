{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e651f-b230-467c-83e1-86ec21f8fe49",
   "metadata": {},
   "source": [
    "# Informe del Proyecto: Clasificación de Imágenes de Espectrogramas con VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c9d1c3-5d39-4752-8f00-5428acccf82c",
   "metadata": {},
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591e63a-f3a0-47ed-9d5d-5188f65607ea",
   "metadata": {},
   "source": [
    "El presente cuadernillo tiene como objetivo desarrollar un modelo de aprendizaje profundo para la clasificación binaria de espectrogramas, utilizando la arquitectura preentrenada VGG16. La tarea principal es identificar si una imagen pertenece a una clase específica, a partir de imágenes espectrales almacenadas en un directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff6166c-9ad7-4dc6-947d-12c3d75095c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e1a1c-9bfa-4515-8e62-18856970386a",
   "metadata": {},
   "source": [
    "## 2.Preparación de los Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd031d8-a17d-42c0-bd90-9ace3e553168",
   "metadata": {},
   "source": [
    "### 2.1 Configuración de Parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61885e-a9fa-4a37-8452-6a50c9fb9728",
   "metadata": {},
   "source": [
    "Se establecieron los siguientes parámetros para el preprocesamiento y la configuración del modelo:\n",
    "\n",
    "*    Altura y Ancho de Imágenes: 128x128 píxeles\n",
    "*    Tamaño de Lote (Batch Size): 32\n",
    "*    Número de Épocas: 5\n",
    "*    Directorio Base de Imágenes: Ruta específica donde se encuentran los espectrogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be424fdb-5061-49b0-91bb-b2f989e32d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters():\n",
    "    params = {\n",
    "        'img_height': 128,\n",
    "        'img_width': 128,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 5,\n",
    "        'learning_rate': 0.0001,\n",
    "        'base_dir': '/home/yvan/Escritorio/Acustic/img/Spectrograms'\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b0a90-fb9c-433c-87ba-30fcb898556f",
   "metadata": {},
   "source": [
    "### 2.2 Generación de Datos de Entrenamiento y Validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506679b9-1295-4333-9521-95392a6d76ce",
   "metadata": {},
   "source": [
    "Se utilizó ImageDataGenerator para realizar aumentación y normalización de imágenes. Las imágenes fueron divididas en un 80% para entrenamiento y un 20% para validación.\n",
    "\n",
    "*    Rescale: Las imágenes fueron normalizadas a un rango de 0,10,1.\n",
    "*    Aumentación: Se implementaron rotaciones, desplazamientos horizontales y verticales, y volteo horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173f98cc-1c9b-41da-b422-29eebddadac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear generadores de datos\n",
    "def create_data_generators(base_dir, img_height, img_width, batch_size):\n",
    "    datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='training'\n",
    "    )\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        base_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary',\n",
    "        subset='validation'\n",
    "    )\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c7298-7e8e-4903-9102-c59c44dd0243",
   "metadata": {},
   "source": [
    "## 3. Diseño del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c206d-b508-43ef-b6a6-88cc88edde0a",
   "metadata": {},
   "source": [
    "### 3.1 Arquitectura del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905fc1b-b59f-4803-8ee4-d4e87c598849",
   "metadata": {},
   "source": [
    "La arquitectura del modelo se basa en VGG16 preentrenado en el conjunto de datos ImageNet, con las siguientes modificaciones:\n",
    "\n",
    "*    Capas Congeladas: Todas las capas de VGG16 se congelaron para evitar la actualización de los pesos preentrenados.\n",
    "*    Capas Personalizadas: Se añadieron capas de aplanado (Flatten), densa (Dense) y de abandono (Dropout) para mejorar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6660507a-58b9-4046-b562-e1c32c7b1ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear y compilar el modelo\n",
    "def create_model(img_height, img_width, learning_rate):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = Flatten()(base_model.output)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', Recall()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a67664e6-9de3-4b21-a932-030a5b38f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "def train_model(model, train_generator, validation_generator, epochs):\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[reduce_lr]\n",
    "    )\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8311dcaf-02c9-45e8-92c9-89824b430600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3540 images belonging to 2 classes.\n",
      "Found 884 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 1s/step - accuracy: 0.8559 - loss: 0.3138 - recall_1: 0.9050 - val_accuracy: 0.8891 - val_loss: 0.2026 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 2/5\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0155 - recall_1: 1.0000 - val_accuracy: 0.9400 - val_loss: 0.1154 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 3/5\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0058 - recall_1: 1.0000 - val_accuracy: 0.9378 - val_loss: 0.1157 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 4/5\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0036 - recall_1: 1.0000 - val_accuracy: 0.9446 - val_loss: 0.1047 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n",
      "Epoch 5/5\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0023 - recall_1: 1.0000 - val_accuracy: 0.9604 - val_loss: 0.0768 - val_recall_1: 1.0000 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "#Ejecución\n",
    "params = get_parameters()\n",
    "train_generator, validation_generator = create_data_generators(\n",
    "    params['base_dir'], params['img_height'], params['img_width'], params['batch_size']\n",
    ")\n",
    "model = create_model(params['img_height'], params['img_width'], params['learning_rate'])\n",
    "history = train_model(model, train_generator, validation_generator, params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd00ba5-e009-43f1-9446-b1bb629c4499",
   "metadata": {},
   "source": [
    "## 4. Análisis de los Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965972d-1beb-4778-bebe-2c7ea8d6a1f5",
   "metadata": {},
   "source": [
    "El modelo fue entrenado durante 5 épocas, utilizando el generador de datos de entrenamiento y validación. Se empleó el callback ReduceLROnPlateau para reducir la tasa de aprendizaje en caso de estancamiento en la mejora de la función de pérdida.\n",
    "\n",
    "Se observa una mejora constante en la precisión del conjunto de entrenamiento con cada época, alcanzando un valor máximo del 99.08% en la última época. Sin embargo, la precisión en el conjunto de validación muestra una mejora más gradual y no alcanza valores tan altos, situándose en torno al 84%.\n",
    "\n",
    "Este comportamiento puede sugerir que el modelo podría estar empezando a sobreajustarse al conjunto de entrenamiento en épocas posteriores. Una pérdida de validación relativamente estable (disminuyendo de 0.3406 a 0.3036) indica que el modelo sigue generalizando adecuadamente, aunque puede ser beneficioso implementar regularización adicional o ajustar parámetros para mejorar la precisión de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ec3dd-c648-4f51-ae8a-2d2c03bdd027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
